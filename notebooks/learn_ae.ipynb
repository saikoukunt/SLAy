{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e6c0e4-7d2c-4b1d-a989-d20d3b1cccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47db8068-cc36-4746-a3db-a77d70e1151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Determine if any GPUs are available\n",
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f539076-024b-4643-813f-54e3d9108e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Convolutional Variational Autoencoder\n",
    "\"\"\"\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, imgChannels=1, featureDim=32*20*20, zDim=10):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(imgChannels, 16, 5)\n",
    "        self.encConv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.encFC1 = nn.Linear(featureDim, zDim)\n",
    "        self.encFC2 = nn.Linear(featureDim, zDim)\n",
    "\n",
    "        # Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "        self.decFC1 = nn.Linear(zDim, featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(32, 16, 5)\n",
    "        self.decConv2 = nn.ConvTranspose2d(16, imgChannels, 5)\n",
    "\n",
    "    def encoder(self, x):\n",
    "\n",
    "        # Input is fed into 2 convolutional layers sequentially\n",
    "        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)\n",
    "        # Mu and logVar are used for generating middle representation z and KL divergence loss\n",
    "        x = F.relu(self.encConv1(x))\n",
    "        x = F.relu(self.encConv2(x))\n",
    "        x = x.view(-1, 32*20*20)\n",
    "        mu = self.encFC1(x)\n",
    "        logVar = self.encFC2(x)\n",
    "        return mu, logVar\n",
    "\n",
    "    def reparameterize(self, mu, logVar):\n",
    "\n",
    "        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
    "        std = torch.exp(logVar/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "        # z is fed back into a fully-connected layers and then into two transpose convolutional layers\n",
    "        # The generated output is the same size of the original input\n",
    "        x = F.relu(self.decFC1(z))\n",
    "        x = x.view(-1, 32, 20, 20)\n",
    "        x = F.relu(self.decConv1(x))\n",
    "        x = torch.sigmoid(self.decConv2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder\n",
    "        # output, mu, and logVar are returned for loss computation\n",
    "        mu, logVar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logVar)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f214c3b6-0869-4d53-92df-67102d7c2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize Hyperparameters\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create dataloaders to feed data into the neural network\n",
    "Default MNIST dataset is used and standard train/test split is performed\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                    transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Initialize the network and the Adam optimizer\n",
    "\"\"\"\n",
    "net = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b87f8aa2-40e5-4cdc-a0a0-ceed00b23c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              move_data         3.14%       1.034ms       100.00%      32.978ms      32.978ms             1  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        62.28%      20.539ms        89.05%      29.366ms      29.366ms             1  \n",
      "                                               aten::to         1.11%     365.000us        15.27%       5.035ms      13.044us           386  \n",
      "                                         aten::_to_copy         3.59%       1.184ms        14.27%       4.705ms      18.307us           257  \n",
      "                                         aten::randperm         4.75%       1.568ms         9.52%       3.138ms       1.569ms             2  \n",
      "                                            aten::copy_         9.46%       3.121ms         9.46%       3.121ms      12.144us           257  \n",
      "                                              aten::div         3.39%       1.117ms         5.91%       1.948ms      15.219us           128  \n",
      "                                           aten::select         3.68%       1.213ms         4.15%       1.367ms       5.340us           256  \n",
      "                                          aten::permute         1.95%     642.000us         2.34%     771.000us       6.023us           128  \n",
      "                                             aten::view         1.45%     479.000us         1.45%     479.000us       3.713us           129  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 32.978ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"move_data\"):\n",
    "        imgs, _ = next(iter(train_loader))\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ba3ed14-2765-41b5-a23f-ca724001a4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27bbf74d-a8b0-4c6d-8f4f-d416610d0a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  net        10.54%       2.949ms       100.00%      27.966ms      27.966ms             1  \n",
      "                    aten::convolution         0.57%     159.000us        38.69%      10.820ms       2.705ms             4  \n",
      "                   aten::_convolution         0.54%     151.000us        38.12%      10.661ms       2.665ms             4  \n",
      "                         aten::conv2d         0.10%      28.000us        24.57%       6.870ms       3.435ms             2  \n",
      "                         aten::linear         0.56%     156.000us        24.10%       6.740ms       2.247ms             3  \n",
      "              aten::cudnn_convolution        23.22%       6.495ms        23.22%       6.495ms       3.248ms             2  \n",
      "                          aten::addmm        20.59%       5.758ms        20.66%       5.778ms       1.926ms             3  \n",
      "               aten::conv_transpose2d         0.06%      18.000us        14.29%       3.996ms       1.998ms             2  \n",
      "    aten::cudnn_convolution_transpose        13.46%       3.765ms        13.46%       3.765ms       1.883ms             2  \n",
      "                           aten::relu         4.02%       1.125ms        10.09%       2.823ms     705.750us             4  \n",
      "-------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 27.966ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"net\"):\n",
    "                out, mu, logVar = net(imgs)\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeea0457-f94b-43a3-8077-a4f03c2c424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     calc_loss        31.70%       2.995ms       100.00%       9.448ms       9.448ms             1  \n",
      "                     aten::pow        21.87%       2.066ms        21.93%       2.072ms       2.072ms             1  \n",
      "    aten::binary_cross_entropy        14.07%       1.329ms        16.96%       1.602ms       1.602ms             1  \n",
      "                     aten::sum        13.46%       1.272ms        13.54%       1.279ms     639.500us             2  \n",
      "                    aten::rsub         5.32%     503.000us        12.48%       1.179ms       1.179ms             1  \n",
      "                     aten::sub         7.15%     676.000us         7.15%     676.000us     676.000us             1  \n",
      "                     aten::mul         2.06%     195.000us         2.06%     195.000us     195.000us             1  \n",
      "                     aten::add         1.40%     132.000us         1.40%     132.000us      44.000us             3  \n",
      "              aten::resize_as_         1.26%     119.000us         1.32%     125.000us     125.000us             1  \n",
      "                   aten::copy_         0.54%      51.000us         0.54%      51.000us      51.000us             1  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 9.448ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harris_Lab\\miniconda3\\envs\\burst-detector\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"calc_loss\"):\n",
    "        kl_divergence = 0.5 * torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "        loss = F.binary_cross_entropy(out, imgs, size_average=False) + kl_divergence\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d67daa-2231-4945-ae88-ec199eba5be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               backprop        49.48%     534.575ms        50.97%     550.673ms     550.673ms             1  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.01%      70.000us        48.19%     520.726ms     130.181ms             4  \n",
      "                                   ConvolutionBackward0         0.01%      77.000us        48.19%     520.656ms     130.164ms             4  \n",
      "                             aten::convolution_backward        48.15%     520.280ms        48.18%     520.579ms     130.145ms             4  \n",
      "                               Optimizer.step#Adam.step         0.53%       5.739ms         1.40%      15.143ms      15.143ms             1  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.01%      58.000us         0.20%       2.175ms     725.000us             3  \n",
      "                                         AddmmBackward0         0.02%     246.000us         0.19%       2.008ms     669.333us             3  \n",
      "                                       aten::zeros_like         0.01%     162.000us         0.18%       1.982ms      70.786us            28  \n",
      "                                    aten::_foreach_add_         0.09%     996.000us         0.16%       1.741ms     870.500us             2  \n",
      "                                               aten::mm         0.16%       1.698ms         0.16%       1.698ms     283.000us             6  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.080s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"backprop\"):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ff47d5-1b92-43f2-88f9-808d6fac4b88",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Feeding a batch of images into the network to obtain the output image, mu, and logVar\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 43\u001b[0m     out, mu, logVar \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalc_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\burst-detector\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# The entire pipeline of the VAE: encoder -> reparameterization -> decoder\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# output, mu, and logVar are returned for loss computation\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     mu, logVar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logVar)\n\u001b[0;32m     54\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mVAE.encoder\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Input is fed into 2 convolutional layers sequentially\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Mu and logVar are used for generating middle representation z and KL divergence loss\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencConv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencConv2(x))\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\burst-detector\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\burst-detector\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\burst-detector\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialize Hyperparameters\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create dataloaders to feed data into the neural network\n",
    "Default MNIST dataset is used and standard train/test split is performed\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                    transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Initialize the network and the Adam optimizer\n",
    "\"\"\"\n",
    "net = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training the network for a given number of epochs\n",
    "The loss after every epoch is printed\n",
    "\"\"\"\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    for epoch in range(5):\n",
    "        running_loss = 0\n",
    "        for idx, data in enumerate(train_loader, 0):\n",
    "            with record_function(\"move_data\"):\n",
    "                imgs, _ = data\n",
    "                imgs = imgs.to(device)\n",
    "\n",
    "            # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "            with record_function(\"net\"):\n",
    "                out, mu, logVar = net(imgs)\n",
    "\n",
    "            # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
    "            with record_function(\"calc_loss\"):\n",
    "                kl_divergence = 0.5 * torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "                loss = F.binary_cross_entropy(out, imgs, size_average=False) + kl_divergence\n",
    "\n",
    "                running_loss += loss\n",
    "\n",
    "            # Backpropagation based on the loss\n",
    "            with record_function(\"backprop\"):\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        print('Epoch {}: Loss {}'.format(epoch, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514571e1-6e81-49c2-b850-fd67a55309e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following part takes a random image from test loader to feed into the VAE.\n",
    "Both the original image and generated image from the distribution are shown.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in random.sample(list(test_loader), 1):\n",
    "        imgs, _ = data\n",
    "        imgs = imgs.to(device)\n",
    "        img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.squeeze(img))\n",
    "        out, mu, logVAR = net(imgs)\n",
    "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(np.squeeze(outimg))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61169fa-c235-4388-a69c-f2799521744a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encConv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (encConv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (encFC1): Linear(in_features=12800, out_features=10, bias=True)\n",
       "  (encFC2): Linear(in_features=12800, out_features=10, bias=True)\n",
       "  (decFC1): Linear(in_features=10, out_features=12800, bias=True)\n",
       "  (decConv1): ConvTranspose2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (decConv2): ConvTranspose2d(16, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052c5606-cd77-41fd-84f0-ab075ac0d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.decoder(torch.randn(10))\n",
    "outimg = out.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b07f35-4cff-49a5-b12a-33c15d2bd5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2434f270430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(outimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d69f6f99-3750-423b-bd0e-1a8e7e888a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50e12f63-20a9-42a0-92dd-e25232778b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24393e-f13b-4784-a8a2-e588ea14a8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
