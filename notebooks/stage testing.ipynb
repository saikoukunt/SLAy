{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from typing import Any, TypeAlias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import burst_detector as bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['data_filepath'] = \"C:/Users/Harris_Lab/Projects/burst-detector/data/rec_bank0_dense_g0/KS2.5/catgt_rec_bank0_dense_g0/rec_bank0_dense_g0_imec0/rec_bank0_dense_g0_tcat.imec0.ap.bin\"\n",
    "params['KS_folder'] = \"../data/rec_bank0_dense_g0/KS2.5/catgt_rec_bank0_dense_g0/rec_bank0_dense_g0_imec0/imec0_ks2/\"\n",
    "params['n_chan'] = 385\n",
    "\n",
    "\n",
    "params[\"dtype\"] = \"int16\"\n",
    "params[\"min_spikes\"] = 100\n",
    "params[\"good_lbls\"] = [\"good\", \"mua\"]\n",
    "params[\"pre_samples\"] = 20\n",
    "params[\"post_samples\"] = 62\n",
    "params[\"max_spikes\"] = 1000\n",
    "params[\"sim_type\"] = \"ae\"\n",
    "params[\"ae_pre\"] = 10\n",
    "params[\"ae_post\"] = 30\n",
    "params[\"ae_chan\"] = 8\n",
    "params[\"ae_shft\"] = False\n",
    "params[\"ae_epochs\"] = 25\n",
    "params[\"sim_thresh\"] = .4\n",
    "\n",
    "params['window_size'] = .025\n",
    "params[\"xcorr_bin_width\"] = 0.0005\n",
    "params[\"max_window\"] = 0.25\n",
    "params[\"min_xcorr_rate\"] = 1200\n",
    "params[\"fs\"] = 30000\n",
    "params[\"overlap_tol\"] = 10 / 30000\n",
    "\n",
    "params['ref_pen_bin_width'] = 1\n",
    "params['max_viol'] = .25\n",
    "params[\"xcorr_coeff\"] = 0.5\n",
    "params[\"ref_pen_coeff\"] = 1\n",
    "params[\"final_thresh\"] = .6\n",
    "params[\"max_dist\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(params[\"KS_folder\"], \"automerge\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(params[\"KS_folder\"], \"automerge\", \"merges\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n"
     ]
    }
   ],
   "source": [
    "# Load sorting and recording info.\n",
    "print(\"Loading files...\")\n",
    "times: NDArray[np.float_] = np.load(\n",
    "    os.path.join(params[\"KS_folder\"], \"spike_times.npy\")\n",
    ").flatten()\n",
    "clusters: NDArray[np.int_] = np.load(\n",
    "    os.path.join(params[\"KS_folder\"], \"spike_clusters.npy\")\n",
    ").flatten()\n",
    "cl_labels: pd.DataFrame = pd.read_csv(\n",
    "    os.path.join(params[\"KS_folder\"], \"cluster_group.tsv\"), sep=\"\\t\"\n",
    ")\n",
    "channel_pos: NDArray[np.float_] = np.load(\n",
    "    os.path.join(params[\"KS_folder\"], \"channel_positions.npy\")\n",
    ")\n",
    "\n",
    "# Compute useful cluster info.\n",
    "n_clust: int = clusters.max() + 1\n",
    "counts: dict[int, int] = bd.spikes_per_cluster(clusters)\n",
    "times_multi: list[NDArray[np.float_]] = bd.find_times_multi(\n",
    "    times, clusters, np.arange(clusters.max() + 1)\n",
    ")\n",
    "\n",
    "# Load the ephys recording.\n",
    "rawData = np.memmap(params[\"data_filepath\"], dtype=params[\"dtype\"], mode=\"r\")\n",
    "data: NDArray[np.int16] = np.reshape(\n",
    "    rawData, (int(rawData.size / params[\"n_chan\"]), params[\"n_chan\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark units that we don't want to consider.\n",
    "cl_good: NDArray[np.bool_] = np.zeros(n_clust, dtype=bool)\n",
    "unique: NDArray[np.int_] = np.unique(clusters)\n",
    "for cl in range(n_clust):\n",
    "    if (\n",
    "        (cl in unique)\n",
    "        and (counts[cl] > params[\"min_spikes\"])\n",
    "        and (\n",
    "            cl_labels.loc[cl_labels[\"cluster_id\"] == cl, \"group\"].item()\n",
    "            in params[\"good_lbls\"]\n",
    "        )\n",
    "    ):\n",
    "        cl_good[cl] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster mean waveforms if needed.\n",
    "spikes: dict[int, NDArray[np.int_]] | None = None\n",
    "try:\n",
    "    mean_wf: NDArray[np.float_] = np.load(\n",
    "        os.path.join(params[\"KS_folder\"], \"mean_waveforms.npy\")\n",
    "    )\n",
    "    std_wf: NDArray[np.float_] = np.load(\n",
    "        os.path.join(params[\"KS_folder\"], \"std_waveforms.npy\")\n",
    "    )\n",
    "except OSError:\n",
    "    print(\n",
    "        \"mean_waveforms.npy doesn't exist, calculating mean waveforms on the fly...\"\n",
    "    )\n",
    "    mean_wf: NDArray[np.float_] = np.zeros(\n",
    "        (n_clust, params[\"n_chan\"], params[\"pre_samples\"] + params[\"post_samples\"])\n",
    "    )\n",
    "    std_wf: NDArray[np.float_] = np.zeros_like(mean_wf)\n",
    "    spikes = {}\n",
    "    for i in range(n_clust):\n",
    "        if cl_good[i]:\n",
    "            spikes[i] = bd.extract_spikes(\n",
    "                data,\n",
    "                times_multi,\n",
    "                i,\n",
    "                n_chan=params[\"n_chan\"],\n",
    "                pre_samples=params[\"pre_samples\"],\n",
    "                post_samples=params[\"post_samples\"],\n",
    "                max_spikes=params[\"max_spikes\"],\n",
    "            )\n",
    "            mean_wf[i, :, :] = np.nanmean(spikes[i], axis=0)\n",
    "            std_wf[i, :, :] = np.nanstd(spikes[i], axis=0)\n",
    "    np.save(os.path.join(params[\"KS_folder\"], \"mean_waveforms.npy\"), mean_wf)\n",
    "    np.save(os.path.join(params[\"KS_folder\"], \"std_waveforms.npy\"), std_wf)\n",
    "peak_chans: NDArray[np.int_] = np.argmax(np.max(mean_wf, 2) - np.min(mean_wf, 2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spike snippets to train autoencoder...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting spike snippets to train autoencoder...\")\n",
    "spk_fld: str = os.path.join(params[\"KS_folder\"], \"automerge\", \"spikes\")\n",
    "ci: dict[str, Any] = {\n",
    "    \"times\": times,\n",
    "    \"times_multi\": times_multi,\n",
    "    \"clusters\": clusters,\n",
    "    \"counts\": counts,\n",
    "    \"labels\": cl_labels,\n",
    "    \"mean_wf\": mean_wf,\n",
    "}\n",
    "ext_params: dict[str, Any] = {\n",
    "    \"spk_fld\": spk_fld,\n",
    "    \"pre_samples\": params[\"ae_pre\"],\n",
    "    \"post_samples\": params[\"ae_post\"],\n",
    "    \"num_chan\": params[\"ae_chan\"],\n",
    "    \"for_shft\": params[\"ae_shft\"],\n",
    "}\n",
    "spk_snips: torch.Tensor\n",
    "cl_ids: NDArray[np.int_]\n",
    "spk_snips, cl_ids = bd.generate_train_data(\n",
    "    data, ci, channel_pos, ext_params, params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training autoencoder...\")\n",
    "# net, spk_data = bd.train_ae(\n",
    "#     spk_snips,\n",
    "#     cl_ids,\n",
    "#     do_shft=params[\"ae_shft\"],\n",
    "#     num_epochs=params[\"ae_epochs\"],\n",
    "# )\n",
    "# torch.save(\n",
    "#     net.state_dict(),\n",
    "#     os.path.join(params[\"KS_folder\"], \"automerge\", \"ae.pt\"),\n",
    "# )\n",
    "# print(\n",
    "#     \"Autoencoder saved in \"\n",
    "#     + str(os.path.join(params[\"KS_folder\"], \"automerge\", \"ae.pt\"))\n",
    "# )\n",
    "params[\"model_path\"] = r\"C:\\Users\\Harris_Lab\\Projects\\burst-detector\\data\\rec_bank0_dense_g0\\KS2.5\\catgt_rec_bank0_dense_g0\\rec_bank0_dense_g0_imec0\\imec0_ks2\\automerge\\ae.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net: bd.CN_AE = bd.CN_AE().to(device)\n",
    "net.load_state_dict(torch.load(params[\"model_path\"]))\n",
    "net.eval()\n",
    "spk_data: bd.SpikeDataset = bd.SpikeDataset(spk_snips, cl_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating latent features...\n",
      "Batch 1022/1022\n",
      "LOSS: tensor(20.2725, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity using distances in the autoencoder latent space.\n",
    "spk_lat_peak: NDArray[np.float_]\n",
    "lat_mean: NDArray[np.float_]\n",
    "spk_lab: NDArray[np.int_]\n",
    "sim, spk_lat_peak, lat_mean, spk_lab = bd.calc_ae_sim(\n",
    "mean_wf, net, peak_chans, spk_data, cl_good, do_shft=params[\"ae_shft\"]\n",
    ")\n",
    "pass_ms = sim > params[\"sim_thresh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating cross-correlation metric...\n"
     ]
    }
   ],
   "source": [
    "# Calculate a significance metric for cross-correlograms.\n",
    "print(\"Calculating cross-correlation metric...\")\n",
    "xcorr_sig: NDArray[np.float_]\n",
    "x_grams: NDArray\n",
    "shfl_xgrams: NDArray\n",
    "xcorr_sig, xgrams, shfl_xgrams = bd.calc_xcorr_metric(\n",
    "    times_multi, n_clust, pass_ms, params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_pen: NDArray[np.float_]\n",
    "ref_per: NDArray[np.float_]\n",
    "ref_pen, ref_per = bd.calc_ref_p(\n",
    "    times_multi, clusters, n_clust, pass_ms, xcorr_sig, params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metric: NDArray[np.float_] = np.zeros_like(sim)\n",
    "for c1 in range(n_clust):\n",
    "    for c2 in range(c1, n_clust):\n",
    "        met: float = (\n",
    "            sim[c1, c2]\n",
    "            + params[\"xcorr_coeff\"] * xcorr_sig[c1, c2]\n",
    "            - params[\"ref_pen_coeff\"] * ref_pen[c1, c2]\n",
    "        )\n",
    "\n",
    "        final_metric[c1, c2] = max(met, 0)\n",
    "        final_metric[c2, c1] = max(met, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "old2new: dict[int, int]\n",
    "new2old: dict[int, list[int]]\n",
    "old2new, new2old = bd.merge_clusters(\n",
    "    clusters, counts, mean_wf, final_metric, params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{457: [370, 367],\n",
       " 458: [105, 389],\n",
       " 459: [436, 350],\n",
       " 460: [432, 337],\n",
       " 461: [392, 438],\n",
       " 462: [434, 451],\n",
       " 463: [398, 165],\n",
       " 464: [139, 148],\n",
       " 465: [395, 440],\n",
       " 466: [404, 223],\n",
       " 467: [160, 60],\n",
       " 468: [439, 134],\n",
       " 469: [119, 120]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new2old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "burst-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
